{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "# BeautifulSoup to scrap the basic landing page\n",
    "from bs4 import BeautifulSoup\n",
    "# Selenium to scrap the individual job posts, and extract the fields populated by scripts\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "# itertools for handling the list of values and transposing\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Initial Page**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first page of the search result for with search input \"python\"\n",
    "URL = 'https://wuzzuf.net/search/jobs/?a=hpb&q=python&start=0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scraping the search result page**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: 1, ads: 15\n",
      "Page: 2, ads: 30\n",
      "Page: 3, ads: 45\n",
      "Page: 4, ads: 60\n",
      "Page: 5, ads: 75\n",
      "Page: 6, ads: 90\n",
      "Page: 7, ads: 105\n",
      "Page: 8, ads: 120\n",
      "Page: 9, ads: 135\n",
      "Page: 10, ads: 143\n",
      "get out\n"
     ]
    }
   ],
   "source": [
    "# Empty container lists, will be filled with the extracted data\n",
    "page = 0\n",
    "companies = []\n",
    "titles = []\n",
    "locations = []\n",
    "skills = []\n",
    "contracts = []\n",
    "links = []\n",
    "\n",
    "# using while loop to iterate through the search pages till there's no more search results\n",
    "# each loop will extract one page data, and fill the data in the container lists\n",
    "while True:\n",
    "    # search result url, changing to match the page number\n",
    "    page_url = f'{URL[:-1]}{page}'\n",
    "    \n",
    "    # Using request and beautiful soup to extract the content of the page\n",
    "    result = requests.get(page_url)\n",
    "    result_content = result.content\n",
    "    soup = BeautifulSoup(result_content, 'lxml')\n",
    "    \n",
    "    # The number of total ads found in the search result\n",
    "    ad_number = int(soup.find('strong').text)\n",
    "\n",
    "    # Extracting the info from the page soup\n",
    "    # result: list of html tag with its text\n",
    "    job_titles = soup.find_all(\"a\", {'class': 'css-o171kl', 'rel': 'noreferrer'})\n",
    "    location_names = soup.find_all('span', {'class': 'css-5wys0k'})\n",
    "    contract_type = soup.find_all('div', {'class': 'css-1lh32fc'})\n",
    "    job_skills = soup.find_all('div', {'class': 'css-y4udm8'})\n",
    "    company_names = soup.find_all(\"a\", {'class': 'css-17s97q8'})\n",
    "\n",
    "    page_len = len(company_names)\n",
    "    \n",
    "    # populating container lists with the text from info extracted from the page\n",
    "    # each for loop fills the data for one job post\n",
    "    for item in range(page_len):\n",
    "        companies.append(company_names[item].text)\n",
    "        links.append(job_titles[item].attrs['href'])\n",
    "        titles.append(job_titles[item].text)\n",
    "        locations.append(location_names[item].text)\n",
    "        skills.append(job_skills[item].text)\n",
    "        contracts.append(contract_type[item].text)\n",
    "    \n",
    "    # Iterate through the search result pages\n",
    "    page += 1\n",
    "    reached_ad = int(soup.find('li', class_='css-8neukt').text.split()[3])\n",
    "    print(f'Page: {page}, ads: {reached_ad}')\n",
    "    \n",
    "    # Exit the while only when scraped posts number matches the max number of search results\n",
    "    if reached_ad >= ad_number:\n",
    "        print('get out')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Selenium for extracting salary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salary field are inside each post page, and is filled using javaScript\n",
    "# can't be extracted using BeautifulSoup, because the field doesn't have any value from the html page\n",
    "# have to run through each post page and extract the salary using selenium\n",
    "# using Option & headless to stop selenium from opening a FireFox window\n",
    "options = webdriver.FirefoxOptions() \n",
    "options.headless= True\n",
    "driver = webdriver.Firefox(options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample Salary**  \n",
    "since extracting using the selenium is very long process, try extracting just the first item as a proof it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "salaries = []\n",
    "\n",
    "for i, link in enumerate(links):\n",
    "    driver.get(link)    \n",
    "    # Extract the salary field using selenium find\n",
    "    salaries.append(driver.find_elements_by_class_name('css-4xky9y')[3].text)\n",
    "\n",
    "    \"\"\"    \n",
    "    # Or we can use soup since we already have the driver with page data filled in\n",
    "    # Extract the salary field using soup find \n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "    print(soup.find_all(\"span\", class_='css-4xky9y')[3].text)\n",
    "    \"\"\"\n",
    "\n",
    "    # Break to extract only sample of the 4 link\n",
    "    # remove the break when extracting all the data\n",
    "    if i == 3:\n",
    "        break\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating the CSV file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file created\n"
     ]
    }
   ],
   "source": [
    "# Header and row values list\n",
    "col_names = ['Title', 'Company', 'Location', 'Contract', 'Skills', 'Link', 'Salaries']\n",
    "col_values = [titles, companies, locations, contracts, skills, links, salaries]\n",
    "\n",
    "# transpose the values list (initial col_values would have each row with the same data)\n",
    "# ex from col_values: 1st row all titles, 2nd row all companies\n",
    "# we want the result to be one value type in each cell of the row, ex: 1st row: 1 title, 1 company, 1 loc ....\n",
    "# using itertools and zip_longest to transpose and handle missing values in the same time\n",
    "col_values2 = list(map(list, itertools.zip_longest(*col_values, fillvalue='not fetched')))\n",
    "\n",
    "# Write the list of values in a CSV file\n",
    "with open('jobs.csv', 'w', newline = '', encoding='UTF8') as f:\n",
    "    wr = csv.writer(f)\n",
    "    wr.writerow(col_names)\n",
    "    wr.writerows(col_values2)\n",
    "print('file created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
